{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Pick-and-Place Environment in Robosuite\n",
    "\n",
    "## The Task\n",
    "\n",
    "Our task is to solve the pick and place environment of the robosuite project with reinforcement learing.\n",
    "Robosuite provides a simulation environment with a box containing the objects, a second box divided into segments for each object and a robotic arm with a gripper as endeffector. \n",
    "The task is considered successfull if the robot manages to place every object into their corresponding segment in the second box. \n",
    "To achieve this goal, the robot has to perform four intermediet tasks.\n",
    "\n",
    "1. Reaching the nearest object.\n",
    "2. Grasping the object.\n",
    "3. Lifting the object out of the box.\n",
    "4. Moving the object to the corresponding segment in the second box.\n",
    "\n",
    "## Parameters of the Robosuite setup\n",
    "\n",
    "todo\n",
    "and reference papers\n",
    "\n",
    "## Stable Baselines 3\n",
    "\n",
    "todo\n",
    "\n",
    "## Reward function\n",
    "\n",
    "The reward function is essential to understand the behaviour of the robot. For our task the reward function is predefined by the environment. For each subtask an additional reẁard is added successively. \n",
    "In this documentation of the reward function describes the rewards for each subtask\n",
    "todo add image\n",
    "![title](\"img/picture.png\")\n",
    "\n",
    "## The Repository\n",
    "Clone the git repository. The jupiter notebook file alone will not provide full functionalities.\n",
    "This jupiter notebook is the main executable and contains all the source code.\n",
    "\n",
    "The ./models/ folder contains the trained models and tensorboard logs, after executing our training script.\n",
    "The ./replay/ folder contains the models and recorded simulations for our later example demonstrations.\n",
    "The ./optuna/ folder contains the optuna logs after executing our optuna training script.\n",
    "The ./prepared_optuna/ folder contains the optuna logs from our 30 hour execution session. \n",
    "\n",
    "## Installation\n",
    "\n",
    "Since robosuite shows complications with windows, a linux or mac computer is required.\n",
    "On debian the non free cuda driver has to be installed as a kernel level module in order to use the GPU for calculations.\n",
    "This change resulted in crashes of wayland DSP so a X11 has to be used as a fallback. \n",
    "\n",
    "Our code is writen for python3.11. The following python packages are needed:\n",
    "numpy (below version 2), robosuite, stable-baselines3[extra], libhdf5, h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install ipywidgets\n",
    "!TMPDIR='/var/tmp' python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import robosuite as suite\n",
    "\n",
    "\n",
    "from robosuite import load_controller_config\n",
    "from robosuite.environments.base import register_env\n",
    "from robosuite.controllers import load_controller_config\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.save_util import save_to_zip_file, load_from_zip_file\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from robosuite.wrappers import GymWrapper\n",
    "\n",
    "from stable_baselines3 import PPO, DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup and testing of parameters\n",
    "\n",
    "At the beginning of the task we trained a few models with different parameters to learn the relations between the parameters and the changes to the model. To do so we created the following script which defines a config dict of the tested parameters at the beginning of the file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    # Environment\n",
    "    robot=\"Panda\",\n",
    "    gripper=\"default\",\n",
    "    controller=\"OSC_POSE\",\n",
    "    seed=12532135,\n",
    "    control_freq=20,\n",
    "    horizon=2048,\n",
    "    camera_size=84,\n",
    "    episodes=200,\n",
    "    n_processes=6,\n",
    "    # Algorithm \n",
    "    algortihm=\"PPO\",\n",
    "    gamma=0.99,\n",
    "    learning_rate=1e-3,\n",
    "    n_steps=2048,\n",
    ")\n",
    "\n",
    "test_name = str(parameters[\"robot\"]) + \"_freq\" + str(parameters[\"robot\"]) + \"_hor\" + str(parameters[\"horizon\"]) + \"_learn\" + str(parameters[\"learning_rate\"]) + \"_episodes\" + str(parameters[\"episodes\"]) + \"_control\" + str(parameters[\"controller\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "The following script will train a model with the previously specified parameters.\n",
    "The model and tensorboard logs will be stored in the \"tests\" folder named according to the specified parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "controller_config = load_controller_config(default_controller=parameters[\"controller\"])\n",
    "\n",
    "def make_env(env_id, options, rank):\n",
    "    def _init():\n",
    "        env = GymWrapper(suite.make(env_id, **options))\n",
    "        env.render_mode = 'mujoco'\n",
    "        env = Monitor(env)\n",
    "        env.reset(seed=parameters[\"seed\"] + rank)\n",
    "        return env\n",
    "    set_random_seed(parameters[\"seed\"])\n",
    "    return _init\n",
    "\n",
    "\n",
    "env = SubprocVecEnv([make_env(\n",
    "    \"PickPlace\",\n",
    "    dict(\n",
    "        robots=[parameters[\"robot\"]],                      \n",
    "        gripper_types=parameters[\"gripper\"],                \n",
    "        controller_configs=controller_config,   \n",
    "        has_renderer=False,                     \n",
    "        has_offscreen_renderer=True,\n",
    "        control_freq=parameters[\"control_freq\"],\n",
    "        horizon=parameters[\"horizon\"],\n",
    "        use_object_obs=False,                       # don't provide object observations to agent\n",
    "        use_camera_obs=True,                        # provide image observations to agent\n",
    "        camera_names=\"agentview\",                   # use \"agentview\" camera for observations\n",
    "        camera_heights=parameters[\"camera_size\"],   # image height\n",
    "        camera_widths=parameters[\"camera_size\"],    # image width\n",
    "        reward_shaping=True),                       # use a dense reward signal for learning\n",
    "    i\n",
    "    ) for i in range(parameters[\"n_processes\"])])\n",
    "\n",
    "\n",
    "env = VecNormalize(env)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma=parameters[\"gamma\"], learning_rate=parameters[\"learning_rate\"], n_steps=parameters[\"n_steps\"], tensorboard_log=test_name, )\n",
    "\n",
    "#env = VecNormalize.load('./' + test_name + '/env.pkl', env)\n",
    "#model = PPO.load(\"./\" + test_name + \"/model.zip\", env=env)\n",
    "\n",
    "model.learn(total_timesteps=parameters[\"horizon\"]*parameters[\"episodes\"], progress_bar=True)\n",
    "model.save(\"./\" + test_name + \"/model.zip\")\n",
    "env.save('./' + test_name + '/env.pkl')\n",
    "\n",
    "env.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "The following command will open a locally hosted http server for the tensorboard.\n",
    "Navigate to http://localhost:6006 to view the data logged during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tensorboard.main --logdir={'./' + test_name + '/'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model\n",
    "With the following script, the model defined by the specified parameters will be used for the task execution. \n",
    "To run a simulation the model with the specified parameters has to be trained first.\n",
    "Therefore adjust the parameters dict and rerun the code block. Then execute the training script. After that the new model can be simulated with the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'parameters' in vars() and not 'parameters' in globals():\n",
    "    print(\"No parameters defined\")\n",
    "elif not os.path.isdir('./' + test_name):\n",
    "    print(\"No model found for this configuration. Train a model first!\")\n",
    "else:\n",
    "    print('Using model ' + test_name)\n",
    "\n",
    "    controller_config = load_controller_config(default_controller=parameters[\"controller\"])\n",
    "\n",
    "    def make_env(env_id, options, rank):\n",
    "        def _init():\n",
    "            env = GymWrapper(suite.make(env_id, **options))\n",
    "            env.render_mode = 'mujoco'\n",
    "            env = Monitor(env)\n",
    "            env.reset(seed=parameters[\"seed\"] + rank)\n",
    "            return env\n",
    "        set_random_seed(parameters[\"seed\"])\n",
    "        return _init\n",
    "\n",
    "\n",
    "    env = SubprocVecEnv([make_env(\n",
    "        \"PickPlace\",\n",
    "        dict(\n",
    "            robots=[parameters[\"robot\"]],                      \n",
    "            gripper_types=parameters[\"gripper\"],                \n",
    "            controller_configs=controller_config,   \n",
    "            has_renderer=True,\n",
    "            has_offscreen_renderer=True,\n",
    "            control_freq=parameters[\"control_freq\"],\n",
    "            horizon=parameters[\"horizon\"],\n",
    "            use_object_obs=False,                       # don't provide object observations to agent\n",
    "            use_camera_obs=True,                        # provide image observations to agent\n",
    "            camera_names=\"agentview\",                   # use \"agentview\" camera for observations\n",
    "            camera_heights=parameters[\"camera_size\"],   # image height\n",
    "            camera_widths=parameters[\"camera_size\"],    # image width\n",
    "            reward_shaping=True),                       # use a dense reward signal for learning\n",
    "        0\n",
    "        )])\n",
    "\n",
    "    #env.render_mode = 'mujoco'\n",
    "    env = VecNormalize.load('./' + test_name + '/env.pkl', env)\n",
    "    model = PPO.load(\"./\" + test_name + \"/model.zip\", env=env)\n",
    "\n",
    "    def get_policy_action(obs):\n",
    "        action, _states = model.predict(obs)\n",
    "        return action\n",
    "\n",
    "    # reset the environment to prepare for a rollout\n",
    "    env.training = False\n",
    "    env.norm_reward = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in range(parameters[\"horizon\"]):\n",
    "        action = get_policy_action(obs)         # use observation to decide on an action\n",
    "        obs, reward, done, info = env.step(action) # play action\n",
    "        env.render()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further developments\n",
    "\n",
    "With these first tests we could test a variety of robot configurations and samples of parameters.\n",
    "We could identify the Sawyer robot with its default gripper and the PPO algorithm as our most promising candidates.\n",
    "\n",
    "Furthermore, we could optimize the first parameters. \n",
    "With the predefined control_freq of the environment of 20 the robot arm was not ä\n",
    "\n",
    "We identified the lack of computing performance as a bottleneck.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "todo some simulation examples with tensorboard graphs and everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay of panda beeing clumsy with its grippers head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay of IIWA bugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay of 20hz control freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay of 100hz control freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay of 250hz control freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n",
    "It is easy to overlook configurations with better performance when multiple parameters correlate. Changes for each parameter on its own reduces performes but adapting parameters to each other easily outperforms the basis configuration. \n",
    "To further increase the performance of the model a wider field of parameters has to be tested. \n",
    "The success of trying random combinations of parameters manually is very limited. Since most of the now relevant parameters are flots, there is a very high number of possible configurations. \n",
    "\n",
    "Optuna is a tool that can automate this task. It samples a value of a range for each parameter for each run. Then it trains a model with this configuration and tests its performacne. This can be done for hundrets of runs unsupervised. The performance for each configuration is logged and can be analysed by the researcher. \n",
    "\n",
    "The \n",
    "\n",
    "### Setup\n",
    "\n",
    "The following script setup and executes 200 runs.\n",
    "To limit the number of tries we already know perform bad, the parameters are fixed values or sampled between boundaries.\n",
    "We took the boundaries for the PPO Algoritm from http://cult...\n",
    "\n",
    "We let optuna run for 30 hours. The logs are also uploaded to this repository. See the next section for the access to the dashboard and our analysis of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robosuite.controllers import load_controller_config\n",
    "from optuna.visualization import plot_optimization_history, plot_slice\n",
    "\n",
    "\n",
    "#optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "#study_name = \"PPO_Sawyer_OSC_POSE\"  # Unique identifier of the study.\n",
    "study_name = \"study1\"\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "\n",
    "# Define the environment setup\n",
    "def make_env(env_id, options, rank, seed=0):\n",
    "    def _init():\n",
    "        env = GymWrapper(suite.make(env_id, **options))\n",
    "        env.render_mode = 'mujoco'\n",
    "        env = Monitor(env)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "def evaluate_policy(model, env, n_eval_episodes=5):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = []\n",
    "        done = np.array([False])\n",
    "        obs = env.reset()\n",
    "        while not done.all():\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "        all_episode_rewards.append(np.sum(episode_rewards))\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward\n",
    "\n",
    "def save_model(model_path, model, vec_env):\n",
    "    model.save(model_path + \".zip\")\n",
    "    vec_env.save(model_path + \".env\")\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    #learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    learning_rate = 0.001\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    gamma = trial.suggest_categorical('gamma', [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    n_steps = trial.suggest_categorical('n_steps', [512, 1024, 2048])\n",
    "    #horizon = trial.suggest_categorical('horizon', [512, 1024, 2048])\n",
    "    horizon = 512\n",
    "    control_freq = trial.suggest_uniform('control_freq', 100, 150)\n",
    "    #total_timesteps = trial.suggest_categorical('total_timesteps', [1e5, 2e5, 5e5, 1e6, 2e6])\n",
    "    total_timesteps = 3e5\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    clip_range = trial.suggest_categorical(\"clip_range\", [0.1, 0.2, 0.3, 0.4])\n",
    "    #n_epochs = trial.suggest_categorical(\"n_epochs\", [1, 5, 10, 20])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5])\n",
    "    vf_coef = trial.suggest_float(\"vf_coef\", 0, 1)\n",
    "    net_arch_type = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\n",
    "\n",
    "    print(\n",
    "        f\"Learning rate: {learning_rate}, \"\n",
    "        f\"Batch size: {batch_size}, \"\n",
    "        f\"Gamma: {gamma}, \"\n",
    "        f\"N steps: {n_steps}, \"\n",
    "        f\"Horizon: {horizon}, \"\n",
    "        f\"Control freq: {control_freq}, \"\n",
    "        f\"Total timesteps: {total_timesteps}, \"\n",
    "        f\"Entropy coefficient: {ent_coef}, \"\n",
    "        f\"Clip range: {clip_range}, \"\n",
    "        f\"GAE lambda: {gae_lambda}, \"\n",
    "        f\"Max grad norm: {max_grad_norm}, \"\n",
    "        f\"Value function coefficient: {vf_coef}, \"\n",
    "        f\"Network architecture: {net_arch_type}\")\n",
    "\n",
    "    # Load configuration\n",
    "    with open(\"config_hyperparams.yaml\") as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "    controller_config = load_controller_config(default_controller=config[\"robot_controller\"])\n",
    "\n",
    "    env_options = {\n",
    "        \"robots\": config[\"robot_name\"],\n",
    "        \"controller_configs\": controller_config,\n",
    "        \"has_renderer\": False,\n",
    "        \"has_offscreen_renderer\": True,\n",
    "        \"single_object_mode\": 2,\n",
    "        \"object_type\": \"milk\",\n",
    "        \"use_camera_obs\": True,\n",
    "        \"use_object_obs\": False,\n",
    "        \"camera_names\": \"agentview\",\n",
    "        \"camera_heights\": 128,\n",
    "        \"camera_widths\": 128,\n",
    "        \"reward_shaping\": True,\n",
    "        \"horizon\": horizon,\n",
    "        \"control_freq\": control_freq,\n",
    "    }    \n",
    "    \n",
    "    # Setup environment\n",
    "    if config[\"multiprocessing\"]:\n",
    "        env = SubprocVecEnv([make_env(\"PickPlace\", env_options, i, config[\"seed\"]) for i in range(config[\"num_envs\"])], start_method='spawn')\n",
    "        eval_env = SubprocVecEnv([make_env(\"PickPlace\", env_options, i, config[\"seed\"]) for i in range(config[\"num_eval_envs\"])], start_method='spawn')\n",
    "        eval_env = VecNormalize(eval_env)\n",
    "        # TODO: account when using multiple envs\n",
    "        if batch_size > n_steps:\n",
    "            batch_size = n_steps\n",
    "    else:\n",
    "        env = DummyVecEnv(make_env(\"PickPlace\", env_options, 0, config[\"seed\"]))\n",
    "        eval_env = DummyVecEnv([make_env(\"PickPlace\", env_options, 0, config[\"seed\"])])\n",
    "        eval_env = VecNormalize(eval_env)\n",
    "\n",
    "    if config[\"normalize\"]:\n",
    "        env = VecNormalize(env)\n",
    "\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Orthogonal initialization\n",
    "    ortho_init = False\n",
    "\n",
    "    activation_fn_name = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Independent networks usually work best\n",
    "    # when not working with images\n",
    "    net_arch = {\n",
    "        \"tiny\": dict(pi=[64], vf=[64]),\n",
    "        \"small\": dict(pi=[64, 64], vf=[64, 64]),\n",
    "        \"medium\": dict(pi=[256, 256], vf=[256, 256]),\n",
    "    }[net_arch_type]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn_name]\n",
    "\n",
    "    # Initialize model\n",
    "    if config[\"algorithm\"] == \"PPO\":\n",
    "        model = PPO(config[\"policy\"],\n",
    "                    env,\n",
    "                    learning_rate=learning_rate,\n",
    "                    batch_size=batch_size,\n",
    "                    gamma=gamma,\n",
    "                    n_steps=n_steps,\n",
    "                    ent_coef=ent_coef,\n",
    "                    clip_range=clip_range,\n",
    "                    gae_lambda=gae_lambda,\n",
    "                    max_grad_norm=max_grad_norm,\n",
    "                    vf_coef=vf_coef,\n",
    "                    policy_kwargs=dict(\n",
    "                                    net_arch=net_arch,\n",
    "                                    activation_fn=activation_fn,\n",
    "                                    ortho_init=ortho_init,\n",
    "                                    ),\n",
    "                    verbose=0,\n",
    "                    tensorboard_log=None,\n",
    "                    device=device\n",
    "                    )\n",
    "    elif config[\"algorithm\"] == \"DDPG\":\n",
    "        n_actions = env.action_space.shape[-1]\n",
    "        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        model = DDPG(config[\"policy\"], env, action_noise=action_noise, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, verbose=0, tensorboard_log=None, device=device)\n",
    "    elif config[\"algorithm\"] == \"SAC\":\n",
    "        model = SAC(config[\"policy\"], env, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, verbose=0, tensorboard_log=None, device=device)\n",
    "    \n",
    "    '''\n",
    "    # Evaluation callback\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
    "                                 log_path=\"./logs/\", eval_freq=2048,\n",
    "                                 deterministic=True, render=False)\n",
    "    '''\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    #mean_reward = eval_callback.last_mean_reward\n",
    "    #mean_reward, _ = model.evaluate_policy(eval_env, n_eval_episodes=5, deterministic=True)\n",
    "    mean_reward = evaluate_policy(model, eval_env, n_eval_episodes=5)\n",
    "    print(\"Mean reward: \", mean_reward)\n",
    "    \n",
    "    trial.report(mean_reward, step=total_timesteps)\n",
    "\n",
    "    #Handle pruning based on the intermediate value\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# Optimize hyperparameters\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50, timeout=90000)\n",
    "\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print('Study statistics: ')\n",
    "    print('  Number of finished trials: ', len(study.trials))\n",
    "    print('  Number of pruned trials: ', len(pruned_trials))\n",
    "    print('  Number of complete trials: ', len(complete_trials))\n",
    "\n",
    "    print('Best trial: ')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: ', trial.value)\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    print('Best hyperparameters: ', study.best_params)\n",
    "    \n",
    "    plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Dashboard\n",
    "Optuna dashboard visualizes the logged results of the optuna execution. \n",
    "The optuna dashboard can be accessed by executing the following command and open https://localhost:port in the browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard command here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "In the _ section we can see that the parameters _, _, _ are especially important for the earned rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Optimized Parameters\n",
    "\n",
    "Let's see how our model with optimized parameters performs. The following script starts a replay of one of our most successfull runs. We can see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
